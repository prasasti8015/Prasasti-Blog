{
  
    
        "post0": {
            "title": "Distinguish Your Own Digits (DYOD)",
            "content": "#importing the necessary libraries %load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . From the command line run pip install mnist. This is a library that will help you bring down the mnist dataset. If you run this from a notebook, you need to put !pip install mnist in a cell by itself. . #this line can be commented out if already installed !pip install mnist . Collecting mnist Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB) Requirement already satisfied: numpy in c: users acer anaconda_3 lib site-packages (from mnist) (1.18.1) Installing collected packages: mnist Successfully installed mnist-0.2.2 . Preparing the Data . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 2 . &lt;matplotlib.image.AxesImage at 0x24bf5468f88&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . #Importing the Kudzu libraries that was provided from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback, ClfCallback from kudzu.model import Model from kudzu.loss import MSE from kudzu.optim import GD from kudzu.layer import Affine, Sigmoid, Relu from kudzu.train import Learner . #Using the following configuration to set up the model for training class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 200 config.bs = 50 . data = Data(X_train, y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . #Initialising the model layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;first&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;second&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;final&quot;, 2, 1), Sigmoid(&quot;final&quot;)] model = Model(layers) . xavier xavier xavier xavier . learner = Learner(loss, model, opt, config.num_epochs) #calling our modified ClfCallback function in the callbacks.py file acc = ClfCallback(learner, config.bs, X_train, X_test, y_train.reshape(-1,1), y_test.reshape(-1,1)) learner.set_callbacks([acc]) . Getting the Epochs and losses as well as the train and test accuracies . learner.train_loop(dl) . Epoch 0 Loss 0.2503608611641938 Train Accuracy 0.629277249207144 Test Accuracy 0.6179435483870968 Epoch 10 Loss 0.08674576363528387 Train Accuracy 0.9187948589550993 Test Accuracy 0.9254032258064516 Epoch 20 Loss 0.05445543360785358 Train Accuracy 0.9420797863461859 Test Accuracy 0.9511088709677419 Epoch 30 Loss 0.042940270344381644 Train Accuracy 0.9526790185277917 Test Accuracy 0.9576612903225806 Epoch 40 Loss 0.037191373402872235 Train Accuracy 0.957603071273577 Test Accuracy 0.9616935483870968 Epoch 50 Loss 0.03365655844497594 Train Accuracy 0.9611083291604072 Test Accuracy 0.9637096774193549 Epoch 60 Loss 0.031226793573225033 Train Accuracy 0.9638624603572025 Test Accuracy 0.9667338709677419 Epoch 70 Loss 0.02938020165613231 Train Accuracy 0.9659489233850777 Test Accuracy 0.9682459677419355 Epoch 80 Loss 0.02790080914026413 Train Accuracy 0.9677015523284928 Test Accuracy 0.9707661290322581 Epoch 90 Loss 0.02666868087329567 Train Accuracy 0.9692872642296778 Test Accuracy 0.9707661290322581 Epoch 100 Loss 0.025617947855054907 Train Accuracy 0.970789517609748 Test Accuracy 0.9712701612903226 Epoch 110 Loss 0.024698540921445213 Train Accuracy 0.971791019863128 Test Accuracy 0.9722782258064516 Epoch 120 Loss 0.02387122491220034 Train Accuracy 0.9725421465531631 Test Accuracy 0.9727822580645161 Epoch 130 Loss 0.0231294017516159 Train Accuracy 0.9739609414121182 Test Accuracy 0.9732862903225806 Epoch 140 Loss 0.02243929724669026 Train Accuracy 0.9748789851443832 Test Accuracy 0.9737903225806451 Epoch 150 Loss 0.021802779924816332 Train Accuracy 0.9752128192288433 Test Accuracy 0.9737903225806451 Epoch 160 Loss 0.021207787625301167 Train Accuracy 0.9757135703555333 Test Accuracy 0.9737903225806451 Epoch 170 Loss 0.0206470243498603 Train Accuracy 0.9762977800033383 Test Accuracy 0.9742943548387096 Epoch 180 Loss 0.020118348556850425 Train Accuracy 0.9773827407778334 Test Accuracy 0.9753024193548387 Epoch 190 Loss 0.019614444139411102 Train Accuracy 0.9780504089467534 Test Accuracy 0.9753024193548387 . 0.03358350741294143 . Plot for the train and test accuracies for the model . plt.figure(figsize= (10,6)) plt.plot(acc.accuracies, &#39;b-&#39;, label = &#39;Train Accuracies&#39;) plt.plot(acc.val_accuracies, &#39;r-&#39;, label = &#39;Test Accuracies&#39;) plt.legend(frameon=False, loc=&#39;lower right&#39;) plt.show() . The above plot shows overfitting of data as the test accuracies comes below the train accuracies. . Plotting of the two-dimensional output before the last Affine with Probability Contours . (hints taken from the TA session videos and Office hour videos) . #taking the model before the last affine where we get the two-dimensional output model_plot = Model(layers[:-2]) plot_data = model_plot(X_test) . #function to plot the data def plotdata(x, y): plt.plot(x[y[:,0] == 1, 0], x[y[:,0] == 1, 1], &#39;ro&#39;, label = &#39;image 3&#39;, alpha= 0.1) plt.plot(x[y[:,0] == 0, 0], x[y[:,0] == 0, 1], &#39;bx&#39;, label = &#39;image 8&#39;, alpha= 0.15) plt.legend(frameon=False, loc=&#39;upper right&#39;) . Calculating the Probability contours . #Inorder to get the probability the last two layers(Logits followed ) are needed to create a model prob_model = Model(layers[-2: ]) . #creating the xx and yy as arrays with 100 rows and 100 columns xgrid = np.linspace(-8, 8, 100) ygrid = np.linspace(-5, 3, 100) xx, yy = np.meshgrid(xgrid, ygrid) . #making both xx and yy as single dimensional arrays and then stacking them vertically and transposing to get the array with 10000 rows and 2 columns X = np.vstack((np.ravel(xx), np.ravel(yy))).T . #The above data with 10000 rows and 2 columns needs to be fed to the probability model that we created to get the probability contours prob_contours = prob_model(X).reshape(100,100) prob_contours.shape . (100, 100) . # Plotting the points with the probability contours plt.figure(figsize = (10,8)) plotdata(plot_data, y_test.reshape(-1,1)) contours = plt.contour(xx, yy, prob_contours) plt.clabel(contours, inline=True, fontsize= 8) . &lt;a list of 6 text.Text objects&gt; . Comparing our Neural Network model with the simple Logistic Regression model . #creating the model for the logistic regression consisting of an Affine and a Sigmoid layer layers_logistic = [Affine(&quot;logits&quot;, 784, 1), Sigmoid(&quot;sigmoid&quot;)] logistic_model = Model(layers_logistic) . xavier . learner_l = Learner(loss, logistic_model, opt, config.num_epochs) #calling our modified ClfCallback function in the callbacks.py file acc_l = ClfCallback(learner_l, config.bs, X_train, X_test, y_train.reshape(-1,1), y_test.reshape(-1,1)) learner_l.set_callbacks([acc_l]) . learner_l.train_loop(dl) . Epoch 0 Loss 0.2367562360959559 Train Accuracy 0.7121515606743448 Test Accuracy 0.7358870967741935 Epoch 10 Loss 0.10001866861793711 Train Accuracy 0.9157903521949591 Test Accuracy 0.9309475806451613 Epoch 20 Loss 0.07784273876847055 Train Accuracy 0.928225671841095 Test Accuracy 0.9410282258064516 Epoch 30 Loss 0.0675107125088896 Train Accuracy 0.9352361876147555 Test Accuracy 0.9506048387096774 Epoch 40 Loss 0.061226968805843864 Train Accuracy 0.9404940744450009 Test Accuracy 0.9551411290322581 Epoch 50 Loss 0.056905649618846574 Train Accuracy 0.9442497078951761 Test Accuracy 0.9571572580645161 Epoch 60 Loss 0.05370901617144848 Train Accuracy 0.9457519612752462 Test Accuracy 0.9576612903225806 Epoch 70 Loss 0.05122610221853336 Train Accuracy 0.9480887998664663 Test Accuracy 0.9581653225806451 Epoch 80 Loss 0.04922616131246544 Train Accuracy 0.9502587214154565 Test Accuracy 0.9596774193548387 Epoch 90 Loss 0.0475721553814584 Train Accuracy 0.9512602236688366 Test Accuracy 0.9616935483870968 Epoch 100 Loss 0.04617537677370344 Train Accuracy 0.9522617259222167 Test Accuracy 0.9611895161290323 Epoch 110 Loss 0.04497324017229766 Train Accuracy 0.9531797696544817 Test Accuracy 0.9616935483870968 Epoch 120 Loss 0.04392529487490612 Train Accuracy 0.9546820230345519 Test Accuracy 0.9632056451612904 Epoch 130 Loss 0.043000719928910185 Train Accuracy 0.9552662326823569 Test Accuracy 0.9642137096774194 Epoch 140 Loss 0.042176491676466914 Train Accuracy 0.9558504423301619 Test Accuracy 0.9652217741935484 Epoch 150 Loss 0.041435937254212546 Train Accuracy 0.956518110499082 Test Accuracy 0.9652217741935484 Epoch 160 Loss 0.04076592825900336 Train Accuracy 0.957269237189117 Test Accuracy 0.9652217741935484 Epoch 170 Loss 0.04015474450440578 Train Accuracy 0.9579369053580371 Test Accuracy 0.9662298387096774 Epoch 180 Loss 0.03959434016758577 Train Accuracy 0.9583541979636121 Test Accuracy 0.9657258064516129 Epoch 190 Loss 0.039078430616818545 Train Accuracy 0.9583541979636121 Test Accuracy 0.9657258064516129 . 0.013997288515574158 . Plotting our Neural Network Model and Logistic Regression model . plt.figure(figsize = (20,5)) plt.subplot(1,2,1) plt.plot(acc.val_accuracies, &#39;b-&#39;, label = &quot;Val Accuracies&quot;) plt.plot(acc.accuracies, &#39;r-&#39;, label = &quot;Accuracies&quot;) plt.title(&quot;Plot for Neural Network Model&quot;) plt.ylim(0.6,1) plt.legend(frameon=False, loc=&#39;lower right&#39;) plt.subplot(1,2,2) plt.plot(acc_l.val_accuracies, &#39;y-&#39;, label = &quot;Val Accuracies&quot;) plt.plot(acc_l.accuracies, &#39;g-&#39;, label = &quot;Accuracies&quot;) plt.title(&quot;Plot for Logistic Regression Model&quot;) plt.ylim(0.6,1) plt.legend(frameon=False, loc=&#39;lower right&#39;) . &lt;matplotlib.legend.Legend at 0x24b8d14dd08&gt; . By comparing the above two graphs it can be seen that our neural network model has better accuracy than the logistic regression model. But in our neural network model overfitting is an issue. .",
            "url": "https://prasasti8015.github.io/Prasasti-Blog/2020/08/09/NeuralNetworkModel.html",
            "relUrl": "/2020/08/09/NeuralNetworkModel.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Covid-19 Dashboard India",
            "content": "India . Last update: 28th July,2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 383723 14164 13882 7717 282 3.62 Tamil Nadu 227688 220716 3659 3571 6972 88 1.61 Delhi 132275 131219 3881 3853 1056 28 2.93 Andhra Pradesh 110297 102349 1148 1090 7948 58 1.04 Karnataka 107001 101465 2064 1962 5536 102 1.93 Uttar Pradesh 73951 70493 1497 1456 3458 41 2.02 West Bengal 62964 60830 1449 1411 2134 38 2.30 Gujarat 57982 56874 2372 2348 1108 24 4.09 Telangana 57142 55532 480 471 1610 9 0.84 Bihar 43591 41111 269 255 2480 14 0.62 Rajasthan 38636 37564 644 633 1072 11 1.67 Assam 34846 33475 92 90 1371 2 0.26 Haryana 32876 32127 406 397 749 9 1.23 Madhya Pradesh 29217 28589 831 821 628 10 2.84 Orissa 28107 26892 189 181 1215 8 0.67 Kerala 20895 19728 68 64 1167 4 0.33 Jammu and Kashmir 18879 18390 333 321 489 12 1.76 Punjab 14378 13769 336 318 609 18 2.34 Jharkhand 9563 8803 94 90 760 4 0.98 Goa 5287 5119 36 36 168 0 0.68 Tripura 4287 4066 21 17 221 4 0.49 Pondicherry 3013 2874 47 43 139 4 1.56 Himachal Pradesh 2330 2270 13 13 60 0 0.56 Manipur 2317 2286 0 0 31 0 0.00 Nagaland 1460 1385 4 5 75 0 0.27 Arunachal Pradesh 1330 1239 3 3 91 0 0.23 Chandigarh 934 910 14 14 24 0 1.50 Meghalaya 779 738 5 5 41 0 0.64 Sikkim 592 568 1 1 24 0 0.17 Mizoram 384 361 0 0 23 0 0.00 Andaman and Nicobar Islands 359 334 1 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://prasasti8015.github.io/Prasasti-Blog/2020/08/06/Covid19Dashboard.html",
            "relUrl": "/2020/08/06/Covid19Dashboard.html",
            "date": " • Aug 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an Embedded Software Engineer at Robert Bosch Engineering and Business Solutions, Pvt. Ltd. Recently I have developed interest in Machine Learning and Artificial Intelligence. That is the reason why I have created this blog to talk about this technology which I find really interesting. . So I will be coming up with posts once in a while. Thank you for the support! .",
          "url": "https://prasasti8015.github.io/Prasasti-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prasasti8015.github.io/Prasasti-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}