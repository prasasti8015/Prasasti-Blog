{
  
    
        "post0": {
            "title": "Distinguish Your Own Digits (DYOD)",
            "content": "#importing the necessary libraries %load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . From the command line run pip install mnist. This is a library that will help you bring down the mnist dataset. If you run this from a notebook, you need to put !pip install mnist in a cell by itself. . #this line can be commented out if already installed !pip install mnist . Collecting mnist Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB) Requirement already satisfied: numpy in c: users acer anaconda_3 lib site-packages (from mnist) (1.18.1) Installing collected packages: mnist Successfully installed mnist-0.2.2 . Preparing the Data . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 2 . &lt;matplotlib.image.AxesImage at 0x24bf5468f88&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . #Importing the Kudzu libraries that was provided from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback, ClfCallback from kudzu.model import Model from kudzu.loss import MSE from kudzu.optim import GD from kudzu.layer import Affine, Sigmoid, Relu from kudzu.train import Learner . #Using the following configuration to set up the model for training class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 200 config.bs = 50 . data = Data(X_train, y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . #Initialising the model layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;first&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;second&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;final&quot;, 2, 1), Sigmoid(&quot;final&quot;)] model = Model(layers) . xavier xavier xavier xavier . learner = Learner(loss, model, opt, config.num_epochs) #calling our modified ClfCallback function in the callbacks.py file acc = ClfCallback(learner, config.bs, X_train, X_test, y_train.reshape(-1,1), y_test.reshape(-1,1)) learner.set_callbacks([acc]) . Getting the Epochs and losses as well as the train and test accuracies . learner.train_loop(dl) . Epoch 0 Loss 0.2503608611641938 Train Accuracy 0.629277249207144 Test Accuracy 0.6179435483870968 Epoch 10 Loss 0.08674576363528387 Train Accuracy 0.9187948589550993 Test Accuracy 0.9254032258064516 Epoch 20 Loss 0.05445543360785358 Train Accuracy 0.9420797863461859 Test Accuracy 0.9511088709677419 Epoch 30 Loss 0.042940270344381644 Train Accuracy 0.9526790185277917 Test Accuracy 0.9576612903225806 Epoch 40 Loss 0.037191373402872235 Train Accuracy 0.957603071273577 Test Accuracy 0.9616935483870968 Epoch 50 Loss 0.03365655844497594 Train Accuracy 0.9611083291604072 Test Accuracy 0.9637096774193549 Epoch 60 Loss 0.031226793573225033 Train Accuracy 0.9638624603572025 Test Accuracy 0.9667338709677419 Epoch 70 Loss 0.02938020165613231 Train Accuracy 0.9659489233850777 Test Accuracy 0.9682459677419355 Epoch 80 Loss 0.02790080914026413 Train Accuracy 0.9677015523284928 Test Accuracy 0.9707661290322581 Epoch 90 Loss 0.02666868087329567 Train Accuracy 0.9692872642296778 Test Accuracy 0.9707661290322581 Epoch 100 Loss 0.025617947855054907 Train Accuracy 0.970789517609748 Test Accuracy 0.9712701612903226 Epoch 110 Loss 0.024698540921445213 Train Accuracy 0.971791019863128 Test Accuracy 0.9722782258064516 Epoch 120 Loss 0.02387122491220034 Train Accuracy 0.9725421465531631 Test Accuracy 0.9727822580645161 Epoch 130 Loss 0.0231294017516159 Train Accuracy 0.9739609414121182 Test Accuracy 0.9732862903225806 Epoch 140 Loss 0.02243929724669026 Train Accuracy 0.9748789851443832 Test Accuracy 0.9737903225806451 Epoch 150 Loss 0.021802779924816332 Train Accuracy 0.9752128192288433 Test Accuracy 0.9737903225806451 Epoch 160 Loss 0.021207787625301167 Train Accuracy 0.9757135703555333 Test Accuracy 0.9737903225806451 Epoch 170 Loss 0.0206470243498603 Train Accuracy 0.9762977800033383 Test Accuracy 0.9742943548387096 Epoch 180 Loss 0.020118348556850425 Train Accuracy 0.9773827407778334 Test Accuracy 0.9753024193548387 Epoch 190 Loss 0.019614444139411102 Train Accuracy 0.9780504089467534 Test Accuracy 0.9753024193548387 . 0.03358350741294143 . Plot for the train and test accuracies for the model . plt.figure(figsize= (10,6)) plt.plot(acc.accuracies, &#39;b-&#39;, label = &#39;Train Accuracies&#39;) plt.plot(acc.val_accuracies, &#39;r-&#39;, label = &#39;Test Accuracies&#39;) plt.legend(frameon=False, loc=&#39;lower right&#39;) plt.show() . The above plot shows overfitting of data as the test accuracies comes below the train accuracies. . Plotting of the two-dimensional output before the last Affine with Probability Contours . (hints taken from the TA session videos and Office hour videos) . #taking the model before the last affine where we get the two-dimensional output model_plot = Model(layers[:-2]) plot_data = model_plot(X_test) . #function to plot the data def plotdata(x, y): plt.plot(x[y[:,0] == 1, 0], x[y[:,0] == 1, 1], &#39;ro&#39;, label = &#39;image 3&#39;, alpha= 0.1) plt.plot(x[y[:,0] == 0, 0], x[y[:,0] == 0, 1], &#39;bx&#39;, label = &#39;image 8&#39;, alpha= 0.15) plt.legend(frameon=False, loc=&#39;upper right&#39;) . Calculating the Probability contours . #Inorder to get the probability the last two layers(Logits followed ) are needed to create a model prob_model = Model(layers[-2: ]) . #creating the xx and yy as arrays with 100 rows and 100 columns xgrid = np.linspace(-8, 8, 100) ygrid = np.linspace(-5, 3, 100) xx, yy = np.meshgrid(xgrid, ygrid) . #making both xx and yy as single dimensional arrays and then stacking them vertically and transposing to get the array with 10000 rows and 2 columns X = np.vstack((np.ravel(xx), np.ravel(yy))).T . #The above data with 10000 rows and 2 columns needs to be fed to the probability model that we created to get the probability contours prob_contours = prob_model(X).reshape(100,100) prob_contours.shape . (100, 100) . # Plotting the points with the probability contours plt.figure(figsize = (10,8)) plotdata(plot_data, y_test.reshape(-1,1)) contours = plt.contour(xx, yy, prob_contours) plt.clabel(contours, inline=True, fontsize= 8) . &lt;a list of 6 text.Text objects&gt; . Comparing our Neural Network model with the simple Logistic Regression model . #creating the model for the logistic regression consisting of an Affine and a Sigmoid layer layers_logistic = [Affine(&quot;logits&quot;, 784, 1), Sigmoid(&quot;sigmoid&quot;)] logistic_model = Model(layers_logistic) . xavier . learner_l = Learner(loss, logistic_model, opt, config.num_epochs) #calling our modified ClfCallback function in the callbacks.py file acc_l = ClfCallback(learner_l, config.bs, X_train, X_test, y_train.reshape(-1,1), y_test.reshape(-1,1)) learner_l.set_callbacks([acc_l]) . learner_l.train_loop(dl) . Epoch 0 Loss 0.2367562360959559 Train Accuracy 0.7121515606743448 Test Accuracy 0.7358870967741935 Epoch 10 Loss 0.10001866861793711 Train Accuracy 0.9157903521949591 Test Accuracy 0.9309475806451613 Epoch 20 Loss 0.07784273876847055 Train Accuracy 0.928225671841095 Test Accuracy 0.9410282258064516 Epoch 30 Loss 0.0675107125088896 Train Accuracy 0.9352361876147555 Test Accuracy 0.9506048387096774 Epoch 40 Loss 0.061226968805843864 Train Accuracy 0.9404940744450009 Test Accuracy 0.9551411290322581 Epoch 50 Loss 0.056905649618846574 Train Accuracy 0.9442497078951761 Test Accuracy 0.9571572580645161 Epoch 60 Loss 0.05370901617144848 Train Accuracy 0.9457519612752462 Test Accuracy 0.9576612903225806 Epoch 70 Loss 0.05122610221853336 Train Accuracy 0.9480887998664663 Test Accuracy 0.9581653225806451 Epoch 80 Loss 0.04922616131246544 Train Accuracy 0.9502587214154565 Test Accuracy 0.9596774193548387 Epoch 90 Loss 0.0475721553814584 Train Accuracy 0.9512602236688366 Test Accuracy 0.9616935483870968 Epoch 100 Loss 0.04617537677370344 Train Accuracy 0.9522617259222167 Test Accuracy 0.9611895161290323 Epoch 110 Loss 0.04497324017229766 Train Accuracy 0.9531797696544817 Test Accuracy 0.9616935483870968 Epoch 120 Loss 0.04392529487490612 Train Accuracy 0.9546820230345519 Test Accuracy 0.9632056451612904 Epoch 130 Loss 0.043000719928910185 Train Accuracy 0.9552662326823569 Test Accuracy 0.9642137096774194 Epoch 140 Loss 0.042176491676466914 Train Accuracy 0.9558504423301619 Test Accuracy 0.9652217741935484 Epoch 150 Loss 0.041435937254212546 Train Accuracy 0.956518110499082 Test Accuracy 0.9652217741935484 Epoch 160 Loss 0.04076592825900336 Train Accuracy 0.957269237189117 Test Accuracy 0.9652217741935484 Epoch 170 Loss 0.04015474450440578 Train Accuracy 0.9579369053580371 Test Accuracy 0.9662298387096774 Epoch 180 Loss 0.03959434016758577 Train Accuracy 0.9583541979636121 Test Accuracy 0.9657258064516129 Epoch 190 Loss 0.039078430616818545 Train Accuracy 0.9583541979636121 Test Accuracy 0.9657258064516129 . 0.013997288515574158 . Plotting our Neural Network Model and Logistic Regression model . plt.figure(figsize = (20,5)) plt.subplot(1,2,1) plt.plot(acc.val_accuracies, &#39;b-&#39;, label = &quot;Val Accuracies&quot;) plt.plot(acc.accuracies, &#39;r-&#39;, label = &quot;Accuracies&quot;) plt.title(&quot;Plot for Neural Network Model&quot;) plt.ylim(0.6,1) plt.legend(frameon=False, loc=&#39;lower right&#39;) plt.subplot(1,2,2) plt.plot(acc_l.val_accuracies, &#39;y-&#39;, label = &quot;Val Accuracies&quot;) plt.plot(acc_l.accuracies, &#39;g-&#39;, label = &quot;Accuracies&quot;) plt.title(&quot;Plot for Logistic Regression Model&quot;) plt.ylim(0.6,1) plt.legend(frameon=False, loc=&#39;lower right&#39;) . &lt;matplotlib.legend.Legend at 0x24b8d14dd08&gt; . By comparing the above two graphs it can be seen that our neural network model has better accuracy than the logistic regression model. But in our neural network model overfitting is an issue. .",
            "url": "https://prasasti8015.github.io/Prasasti-Blog/2020/08/09/NeuralNetworkModel.html",
            "relUrl": "/2020/08/09/NeuralNetworkModel.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Covid-19 Dashboard India",
            "content": "India . Last update: 28th July,2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 383723 14164 13882 7717 282 3.62 Tamil Nadu 227688 220716 3659 3571 6972 88 1.61 Delhi 132275 131219 3881 3853 1056 28 2.93 Andhra Pradesh 110297 102349 1148 1090 7948 58 1.04 Karnataka 107001 101465 2064 1962 5536 102 1.93 Uttar Pradesh 73951 70493 1497 1456 3458 41 2.02 West Bengal 62964 60830 1449 1411 2134 38 2.30 Gujarat 57982 56874 2372 2348 1108 24 4.09 Telangana 57142 55532 480 471 1610 9 0.84 Bihar 43591 41111 269 255 2480 14 0.62 Rajasthan 38636 37564 644 633 1072 11 1.67 Assam 34846 33475 92 90 1371 2 0.26 Haryana 32876 32127 406 397 749 9 1.23 Madhya Pradesh 29217 28589 831 821 628 10 2.84 Orissa 28107 26892 189 181 1215 8 0.67 Kerala 20895 19728 68 64 1167 4 0.33 Jammu and Kashmir 18879 18390 333 321 489 12 1.76 Punjab 14378 13769 336 318 609 18 2.34 Jharkhand 9563 8803 94 90 760 4 0.98 Goa 5287 5119 36 36 168 0 0.68 Tripura 4287 4066 21 17 221 4 0.49 Pondicherry 3013 2874 47 43 139 4 1.56 Himachal Pradesh 2330 2270 13 13 60 0 0.56 Manipur 2317 2286 0 0 31 0 0.00 Nagaland 1460 1385 4 5 75 0 0.27 Arunachal Pradesh 1330 1239 3 3 91 0 0.23 Chandigarh 934 910 14 14 24 0 1.50 Meghalaya 779 738 5 5 41 0 0.64 Sikkim 592 568 1 1 24 0 0.17 Mizoram 384 361 0 0 23 0 0.00 Andaman and Nicobar Islands 359 334 1 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://prasasti8015.github.io/Prasasti-Blog/2020/08/06/Covid19Dashboard.html",
            "relUrl": "/2020/08/06/Covid19Dashboard.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://prasasti8015.github.io/Prasasti-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an Embedded Software Engineer at Robert Bosch Engineering and Business Solutions, Pvt. Ltd. Recently I have developed interest in Machine Learning and Artificial Intelligence. That is the reason why I have created this blog to talk about this technology which I find really interesting. . So I will be coming up with posts once in a while. Thank you for the support! .",
          "url": "https://prasasti8015.github.io/Prasasti-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prasasti8015.github.io/Prasasti-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}